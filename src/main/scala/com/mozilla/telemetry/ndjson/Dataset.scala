/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
package com.mozilla.telemetry.ndjson

import java.time.{LocalDate, ZonedDateTime}
import java.time.format.DateTimeFormatter
import java.time.format.DateTimeFormatter.{ISO_DATE, ISO_DATE_TIME}

import com.mozilla.telemetry.heka
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types.{StringType, StructField, StructType}
import org.json4s.JValue
import org.json4s.JsonAST.{JInt, JObject, JString}
import org.json4s.jackson.JsonMethods.parseOpt

import scala.util.Try
import scalaj.http.Base64

class Dataset private (dataset: String, submissionDate: Option[String], bucket: String, clauses: Map[String, PartialFunction[String, Boolean]]) {
  private val NDJSON_SCHEMA = StructType(Array(
    StructField("payload", StringType, nullable=true),
    StructField("attributeMap", StructType(Array(
      StructField("document_namespace", StringType, nullable=true),
      StructField("document_version", StringType, nullable=true),
      StructField("document_type", StringType, nullable=true),
      StructField("normalized_app_name", StringType, nullable=true),
      StructField("normalized_channel", StringType, nullable=true),
      StructField("app_version", StringType, nullable=true)
    )), nullable=true)
  ))

  private val ATTRIBUTE_TO_DIMENSION = Map(
    "document_namespace" -> "sourceName",
    "document_type" -> "docType",
    "document_version" -> "sourceVersion",
    "normalized_app_name" -> "appName",
    "normalized_channel" -> "appUpdateChannel",
    "app_version" -> "appVersion"
  )

  def asHeka: heka.Dataset = {
    val hekaDataset = clauses.foldLeft(heka.Dataset(dataset)) {
      case (ds, (attr, clause)) => ds.where(ATTRIBUTE_TO_DIMENSION(attr))(clause)
    }
    submissionDate match {
      case Some(expect) => hekaDataset.where("submissionDate") {case date if date == expect => true}
      case _ => hekaDataset
    }
  }

  def where(attribute: String)(clause: PartialFunction[String, Boolean]): Dataset = {
    if (clauses.contains(attribute)) {
      throw new Exception(s"There should be only one clause for $attribute")
    }

    if (attribute == "submissionDate") {
      throw new Exception("submissionDate must only be specified to the ndjson.Dataset constructor")
    }

    Dataset(dataset, submissionDate, bucket, clauses + (attribute -> clause))
  }

  def records(limit: Option[Int] = None)(implicit spark: SparkSession): RDD[Option[JValue]] = {
    // add dashes to submissionDate for backwards compatibility
    val isoSubmissionDate = submissionDate.map(LocalDate.parse(_, Dataset.DATE_NO_DASH).format(ISO_DATE))

    // look up relevant sink output
    val prefix = dataset match {
      case "telemetry" => s"$bucket/telemetry-decoded_gcs-sink/output"
    }

    // TODO move document_namespace and document_type to path
    // TODO use hive partition format for filterable path elements and allow submissionDate as a clause
    val messages = spark.read.schema(NDJSON_SCHEMA)
      .json(s"$prefix/${isoSubmissionDate.getOrElse("*")}/*/*.ndjson.gz")
      .selectExpr("payload", "attributeMap.*")

    val limited = limit match {
      case Some(x) => messages.limit(x)
      case _ => messages
    }

    val filtered = clauses.foldLeft(limited)((df, pair) => {
      val (attribute, clause) = pair
      df.filter(r => {
        val value = r.getAs[String](attribute)
        clause.isDefinedAt(value) && clause(value)
      })
    })

    filtered.rdd.map(
      r => Try(Base64.decodeString(r.getAs[String]("payload"))).toOption.flatMap(parseOpt(_))
    ).map {
      case Some(doc) =>
        // submission_timestamp is generated by the edge server and must be 'an ISO 8601 timestamp with microseconds and timezone "Z"'
        // https://github.com/mozilla/gcp-ingestion/blob/master/docs/edge.md#edge-server-pubsub-message-schema
        val JString(st) = doc \ "submission_timestamp"
        val submissionTimestamp = ZonedDateTime.parse(st, ISO_DATE_TIME)
        // provide doc \ "meta" to better match com.mozilla.telemetry.heka.Message.toJValue
        Some(doc ++ JObject(List(
          ("meta", JObject(List(
            ("submissionDate", JString(submissionTimestamp.format(Dataset.DATE_NO_DASH))),
            ("Timestamp", JInt(
              submissionTimestamp.toEpochSecond * 1e9.toLong + submissionTimestamp.getNano.toLong
            )),
            ("documentId", doc \ "document_id"),
            ("clientId", doc \ "clientId"),
            ("sampleId", doc \ "sample_id"),
            ("appUpdateChannel", doc \ "metadata" \ "uri" \ "app_update_channel"),
            ("normalizedChannel", doc \ "normalized_channel"),
            ("normalizedOSVersion", doc \ "normalized_os_version"),
            ("Date", doc \ "metadata" \ "header" \ "date"),
            ("geoCountry", doc \ "metadata" \ "geo" \ "country"),
            ("geoCity", doc \ "metadata" \ "geo" \ "city"),
            ("geoSubdivision1", doc \ "metadata" \ "geo" \ "subdivision1"),
            ("geoSubdivision2", doc \ "metadata" \ "geo" \ "subdivision2")
          )))
        )))
      case _ => None
    }
  }
}

object Dataset {
  val DATE_NO_DASH: DateTimeFormatter = DateTimeFormatter.ofPattern("yyyyMMdd")
  val BUCKET: String = "gs://moz-fx-data-prod-data"

  def apply(dataset: String, submissionDate: Option[String] = None, bucket: String = Dataset.BUCKET,
            clauses: Map[String, PartialFunction[String, Boolean]]): Dataset = {
    new Dataset(dataset, submissionDate, bucket, clauses)
  }

  implicit def datasetToRDD(dataset: Dataset)(implicit spark: SparkSession): RDD[Option[JValue]] = {
    dataset.records()
  }
}
