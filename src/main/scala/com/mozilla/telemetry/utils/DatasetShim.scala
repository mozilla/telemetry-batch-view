/* This Source Code Form is subject to the terms of the Mozilla Public
 * License, v. 2.0. If a copy of the MPL was not distributed with this
 * file, You can obtain one at http://mozilla.org/MPL/2.0/. */
package com.mozilla.telemetry.utils

import java.io.ByteArrayInputStream
import java.time.format.DateTimeFormatter
import java.time.format.DateTimeFormatter.{ISO_DATE, ISO_DATE_TIME}
import java.time.{LocalDate, ZonedDateTime}
import java.util.zip.GZIPInputStream

import com.mozilla.telemetry.heka
import org.apache.commons.io.IOUtils.toByteArray
import org.apache.spark.SparkContext
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.functions.{col, decode, lit, udf}
import org.apache.spark.sql.{DataFrame, SparkSession}
import org.json4s.JValue
import org.json4s.JsonAST.{JInt, JObject, JString}
import org.json4s.jackson.JsonMethods.parse

class DatasetShim private(dataset: String, attributes: Map[String, String]) {
  private val ATTRIBUTE_TO_DIMENSION = Map(
    "submission_date" -> "submissionDate",
    "document_namespace" -> "sourceName",
    "document_type" -> "docType",
    "document_version" -> "sourceVersion",
    "normalized_app_name" -> "appName",
    "normalized_channel" -> "appUpdateChannel",
    "app_version" -> "appVersion"
  )

  def fromHeka(limit: Option[Int] = None, numPartitions: Option[Int] = None)(implicit sc: SparkContext): RDD[Option[JValue]] = {
    attributes.foldLeft(heka.Dataset(dataset)) {
      case (ds, (attr, expect)) => ds.where(ATTRIBUTE_TO_DIMENSION(attr)){case actual => actual == expect}
    }.records(limit, numPartitions).map(_.toJValue)
  }

  def fromExport(format: String, path: String, limit: Option[Int] = None)(implicit spark: SparkSession): RDD[Option[JValue]] = {
    val messages: DataFrame = spark.read.format(format).load(path)

    val limited = limit match {
      case Some(x) => messages.limit(x)
      case _ => messages
    }

    // udf for decompressing payload
    val gunzip = udf((value: Array[Byte]) => toByteArray(new GZIPInputStream(new ByteArrayInputStream(value))))

    limited
      .selectExpr("*", "metadata.uri.app_version")
      .where(attributes.foldLeft(lit(true)){
        case (condition, (attribute, expect)) if attribute == "submission_date" =>
          condition.and(col(attribute).equalTo(lit(LocalDate.parse(expect, DatasetShim.DATE_NO_DASH).format(ISO_DATE))))
        case (condition, (attribute, expect)) =>
          condition.and(col(attribute).equalTo(lit(expect)))
      })
      .select(decode(gunzip(col("payload")), "UTF-8"))
      .rdd
      .map(row => parse(row.getString(0)))
      .map(doc => {
        // submission_timestamp is generated by the edge server and must be 'an ISO 8601 timestamp with microseconds and timezone "Z"'
        // https://mozilla.github.io/gcp-ingestion/architecture/edge_service_specification/#general-data-flow
        val JString(st) = doc \ "submission_timestamp"
        val submissionTimestamp = ZonedDateTime.parse(st, ISO_DATE_TIME)
        // provide doc \ "meta" to better match com.mozilla.telemetry.heka.Message.toJValue
        Some(doc ++ JObject(List(
          ("meta", JObject(List(
            ("submissionDate", JString(submissionTimestamp.format(DatasetShim.DATE_NO_DASH))),
            ("Timestamp", JInt(
              submissionTimestamp.toEpochSecond * 1e9.toLong + submissionTimestamp.getNano.toLong
            )),
            ("documentId", doc \ "document_id"),
            ("clientId", doc \ "clientId"),
            ("sampleId", doc \ "sample_id"),
            ("appUpdateChannel", doc \ "metadata" \ "uri" \ "app_update_channel"),
            ("normalizedChannel", doc \ "normalized_channel"),
            ("normalizedOSVersion", doc \ "normalized_os_version"),
            ("Date", doc \ "metadata" \ "header" \ "date"),
            ("geoCountry", doc \ "metadata" \ "geo" \ "country"),
            ("geoCity", doc \ "metadata" \ "geo" \ "city"),
            ("geoSubdivision1", doc \ "metadata" \ "geo" \ "subdivision1"),
            ("geoSubdivision2", doc \ "metadata" \ "geo" \ "subdivision2")
          )))
        )))
      })
  }
}

object DatasetShim {
  val DATE_NO_DASH: DateTimeFormatter = DateTimeFormatter.ofPattern("yyyyMMdd")

  def apply(dataset: String, attributes: Map[String, String]): DatasetShim = {
    new DatasetShim(dataset, attributes)
  }
}
