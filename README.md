[![Build Status](https://travis-ci.org/mozilla/telemetry-batch-view.svg?branch=master)](https://travis-ci.org/mozilla/telemetry-batch-view)

# telemetry-batch-view

This is a Scala "framework" to build derived datasets, also known as [batch views](http://robertovitillo.com/2016/01/06/batch-views/), of [Telemetry](https://wiki.mozilla.org/Telemetry) data.

Raw JSON [pings](https://ci.mozilla.org/job/mozilla-central-docs/Tree_Documentation/toolkit/components/telemetry/telemetry/pings.html) are stored on S3 within files containing [framed Heka records](https://hekad.readthedocs.org/en/latest/message/index.html#stream-framing). Reading the raw data in through e.g. Spark can be slow as for a given analysis only a few fields are typically used; not to mention the cost of parsing the JSON blobs. Furthermore, Heka files might contain only a handful of records under certain circumstances.

Defining a derived [Parquet](https://parquet.apache.org/) dataset, which uses a columnar layout optimized for analytics workloads, can drastically improve the performance of analysis jobs while reducing the space requirements. A derived dataset might, and should, also perform heavy duty operations common to all analysis that are going to read from that dataset.

The converted datasets are stored in the bucket specified in [*application.conf*](https://github.com/vitillo/aws-lambda-parquet/blob/master/src/main/resources/application.conf#L2).

### Adding a new derived dataset

See the [streams](https://github.com/vitillo/telemetry-parquet/blob/master/src/main/scala/streams) folder for the currently defined datasets.

### Local execution
Given a subtype of `DerivedStream` of type `MyStream`, a dataset for e.g. the 28th of October can be generated with:
```
sbt "run-main telemetry.DerivedStream --from-date 20151028 --to-date 20151028 MyStream"
```

### Distributed execution
Build an uber-jar with:
```
sbt assembly
```
then, submit the job with:
```
spark-submit --master yarn-client --class telemetry.DerivedStream target/scala-2.10/telemetry-batch-view-X.Y.jar --from-date 20151028 --to-date 20151028 MyStream
``` 

### Running the test suite
```
sbt test
```

### Caveats
If you run into memory issues during compilation time issue the following command before running sbt:
```
export JAVA_OPTIONS="-Xss128M -Xmx2048M" 
```

The Longitudinal Derived Stream
-------------------------------

The longitudinal dataset is generated by `src/main/scala/streams/Longitudinal.scala`, driven by `src/main/scala/DerivedStream.scala`. It's useful for running fast queries on common fields in pings, such as user OSs, channels, and settings.

The resulting dataset can be retrieved as a [DataFrame](https://spark.apache.org/docs/1.3.1/api/java/org/apache/spark/sql/DataFrame.html), which can be thought of as a distributed array of map of arrays:

* The outer array (which is actually a DataFrame) contains one element per client (clients are distinguished by the `clientId` top-level field in pings).
* Each element of that outer array is a map, where fields are the ones defined in `src/main/scala/streams/Longitudinal.scala` in the `buildSchema` function. Remember that the map represents a single client.
  * Whenever records are converted from JSON, JSON keys are converted from camelCase (in the original ping) into snake_case (in the resulting dataset) by `src/main/scala/avro/JSON2Avro.scala`.
  * For example, the `firstPaint` simple measure in a ping object like `(payload.simpleMeasurements -> {"firstPaint":2114,...other simple measurements...},...other fields..)` shows up under `first_paint` in the resulting dataset.
  * The `buildRecord` function in `src/main/scala/streams/Longitudinal.scala` defines exactly how fields in the pings map to fields in the schema.
* Most values in the map are themselves arrays, where elements are the values of that field in the payloads sent by that client.
  * For example, the `geo_country` field will be an array of string country codes, representing the `geoCountry` top-level field in each of the pings sent by that client.
  * Certain fields, like `client_id` (representing `clientId` in the original pings) are not arrays, because they are always the same for a given client.

Note that the pings seen by the code are not exactly the raw pings you see in the [Gecko docs](http://gecko.readthedocs.org/en/latest/toolkit/components/telemetry/telemetry/pings.html). Instead, they're the output of the [data pipeline](https://github.com/mozilla-services/data-pipeline). An example of one of these pings can be found [here](https://gist.github.com/Uberi/686fb2b475ed5924c40b).